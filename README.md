This project is a Retrieval-Augmented Generation (RAG) chatbot built using LangChain, OpenAI, and Streamlit.
It retrieves answers from custom documents (like scraped articles or FAQs), enhances them with LLM responses, and provides conversational memory.

ğŸš€ Built with:
Python
LangChain (Retrieval + Chains)
FAISS (Vector database)
OpenAI Embeddings & LLMs
Streamlit (UI)
Custom Ranking Layer (optional, to improve retrieval relevance)

âš™ï¸ Features
Scrapes and ingests documents into FAISS vector database
Uses text-embedding-3-small for embeddings (cheap & efficient)
Conversational memory with LangChainâ€™s ConversationBufferMemory
Ranking toggle: compare retrieval with vs. without re-ranking
Streamlit UI for local testing


ğŸ”‘ Setup

Clone the repo
git clone <>
cd RAG-FAQ-bot


Create and activate a virtual environment
python -m venv .venv
source .venv/bin/activate   # Mac/Linux
.venv\Scripts\activate      # Windows

Install dependencies
pip install -r requirements.txt

Set your OpenAI API key
Create a .env file in the root folder:

OPENAI_API_KEY=your_openai_api_key_here

ğŸ› ï¸ Usage
1. Scrape data
python src/scraper.py

2. Build embeddings + FAISS index
python src/ingest.py

3. Run the chatbot
streamlit run src/app.py

Then open ğŸ‘‰ http://localhost:8501 in your browser.

ğŸ’¡ Key Learnings
Document chunking with RecursiveCharacterTextSplitter improves retrieval
text-embedding-3-small is efficient for embeddings
Ranking (via reranker models) noticeably boosts answer precision
Adding conversational memory makes interactions feel more natural